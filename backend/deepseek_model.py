# -*- coding: utf-8 -*-
"""deepseek-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdRbLQn_mTxKE__r0QXaU6WTYK7sMfOf
"""

from transformers import LlamaForCausalLM
import torch

# Check if GPU is available and set the device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model
model = LlamaForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-1.3b-base").to(device)
print("âœ… Model loaded successfully!")

from transformers import AutoTokenizer

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-1.3b-base")
print("âœ… Tokenizer loaded successfully!")

# Add FIM special tokens
tokenizer.add_special_tokens({
    "additional_special_tokens": ["<fim_prefix>", "<fim_middle>", "<fim_suffix>"]
})
model.resize_token_embeddings(len(tokenizer))
print("âœ… FIM special tokens added!")

!pip install fastapi uvicorn pyngrok nest-asyncio



!ngrok config add-authtoken 2xbFQdpm49ct8pZBSE2x3p0gQfC_48D2gZCVoQdsKQohFZaY5

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
from transformers import LlamaForCausalLM, AutoTokenizer
import torch

from pyngrok import ngrok
import nest_asyncio
import uvicorn

nest_asyncio.apply()

app = FastAPI()

origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "API is live and ready!"}

# Request model to support both normal and FIM mode
class PromptRequest(BaseModel):
    prompt: Optional[str] = None  # for normal completion
    prefix: Optional[str] = None  # for FIM
    suffix: Optional[str] = None  # for FIM
    language: Optional[str] = None
    mode: Optional[str] = "normal"  # either "normal" or "fim"

# Optional: basic keyword-based language detection
def detect_language_from_code(code: str) -> str:
    code_lower = code.lower()

    if "def " in code_lower and "import" in code_lower:
        return "python"
    elif "#include" in code_lower or "int main()" in code_lower:
        return "c"
    elif "public class" in code_lower or "System.out" in code_lower:
        return "java"
    elif "<html>" in code_lower or "<!DOCTYPE html>" in code_lower:
        return "html"
    elif "console.log" in code_lower or "function(" in code_lower:
        return "javascript"
    elif "#include" in code_lower and "std::" in code_lower:
        return "cpp"

    return "plaintext"

@app.post("/generate")
def generate_code(data: PromptRequest):
    try:
        if data.mode == "fim":
            # FIM Mode
            if data.prefix is None or data.suffix is None:
                return {"error": "For FIM mode, both prefix and suffix must be provided."}

            fim_prompt = f"<fim_prefix>{data.prefix}<fim_middle><fim_suffix>{data.suffix}"
            print(f"FIM prompt sent to model:\n{fim_prompt}")

            inputs = tokenizer(fim_prompt, return_tensors="pt").to(device)

        else:
            # Normal Mode (default)
            if data.language:
                full_prompt = f"# Language: {data.language}\n# Prompt: {data.prompt}\n"
            else:
                full_prompt = f"# Prompt: {data.prompt}\n"

            print(f"Full prompt sent to model:\n{full_prompt}")

            inputs = tokenizer(full_prompt, return_tensors="pt").to(device)

        # Generate
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=1000,
                pad_token_id=tokenizer.eos_token_id
            )

        result = tokenizer.decode(output[0], skip_special_tokens=True)
        print(f"Generated code:\n{result}")

        language_used = data.language or detect_language_from_code(result)

        return {
            "generated_code": result,
            "language": language_used
        }

    except Exception as e:
        print(f"Error: {e}")
        return {"error": str(e)}

# Connect ngrok to port 8000
public_url = ngrok.connect(8000)
print(f"ðŸš€ Your public API is live at: {public_url}")

# Run the FastAPI app on port 8000|
uvicorn.run(app, port=8000)

